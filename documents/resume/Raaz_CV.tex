\documentclass[margin,centered]{res}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{color,xcolor,fdsymbol,enumitem}
\usepackage{fancyhdr}
\usepackage{titling}
\usepackage{fontawesome}
% \author{You}

% \usepackage{fancyhdr}
% \pagestyle{fancy}
% \fancyhf{}
% \makeatletter
% \fancyhead[L]{\textbf{\expandafter\@gobble\@name}}
% \makeatother
% \fancyhead[R]{\thepage}
% \fancyheadoffset[L]{\sectionwidth}
% \setlength\headheight{0pt}
% \setlength\headsep{20pt}
% \addtolength\topmargin{-20pt}
% \AtBeginDocument{\thispagestyle{empty}}

% \pagestyle{fancy}
% \fancyhead{}
% \fancyhead[R]{Raaz Dwivedi}
% \fancyfoot{}
% \fancyfoot[C]{\thepage}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=black,
    pdftitle={Raaz's CV},
    bookmarks=true,
    pdfpagemode=FullScreen,
}
\newcommand\quelle[1]{{%
      \unskip\nobreak\hfil\penalty50
      \hskip2em\hbox{}\nobreak\hfil\emph{#1}%
      \parfillskip=0pt \finalhyphendemerits=0 \par}}


\oddsidemargin -.5in
\evensidemargin -.5in
\textwidth=6.0in
\itemsep=0in
\parsep=0in
% if using pdflatex:
%\setlength{\pdfpagewidth}{\paperwidth}
%\setlength{\pdfpageheight}{\paperheight} 

\newenvironment{list1}{
  \begin{list}{\ding{113}}{%
      \setlength{\itemsep}{0in}
      \setlength{\parsep}{0in} \setlength{\parskip}{0in}
      \setlength{\topsep}{0in} \setlength{\partopsep}{0in} 
      \setlength{\leftmargin}{0.17in}}}{\end{list}}
\newenvironment{list2}{
  \begin{list}{$\bullet$}{%
      \setlength{\itemsep}{0in}
      \setlength{\parsep}{0in} \setlength{\parskip}{0in}
      \setlength{\topsep}{0in} \setlength{\partopsep}{0in} 
      \setlength{\leftmargin}{0.2in}}}{\end{list}}
      
\long\def\comment#1{}

\newcommand{\ptitle}[1]{``#1''}

\begin{document}
% \pagestyle{fancy}
% \thispagestyle{empty}

% \begin{center}
% \name{\centering \sc {Raaz Dwivedi} hi \vspace*{.05in}}  
% \end{center}
% \name{\centering \sc {Raaz Dwivedi} hi \vspace*{.05in}}

\name{\Large \sc{Raaz Dwivedi}}
% \address{A}
% \address{B}
\address{
\\[-3mm]
{\faMobile~\href{tel:15018331977}{\small{(510) 833-1977}}
\quad
\href{mailto:raaz@seas.harvard.edu}{{\footnotesize \faEnvelope}~\small{raaz@seas.harvard.edu}}
\quad
\href{mailto:raaz@mit.edu}{{\footnotesize \faEnvelope}~\small{raaz@mit.edu}}
\quad
\href{https://rzrsk.github.io}{{\faHome}~\small
{rzrsk.github.io}}
% \quad
% \href{https://scholar.google.com/citations?user=9ehX_58AAAAJ&hl=en}{\large
% \faGoogle}
% \quad
% \href{https://github.com/rzrsk}{\large
% \faGithubSquare}
% \quad
% \href{https://www.linkedin.com/in/raaz-dwivedi}{\large
% \faLinkedinSquare}
}
}
% \address{A}

\begin{resume}
% \section{\sc Contact information}
% \vspace{.05in}
% \begin{tabular}{ll}          
% Department of Electrical Engineering \& Computer Sciences & \faEnvelope\ \href{mailto:raaz.rsk@berkeley.edu}{\texttt{raaz.rsk@berkeley.edu}}\\
% University of California, Berkeley  & \href{https://people.eecs.berkeley.edu/~raaz.rsk}
% {\texttt{Homepage}}\\
%     &  \href{https://scholar.google.com/citations?user=9ehX_58AAAAJ&hl=en}
% {\texttt{Google Scholar Page}} \\
% Berkeley, California 94705, USA  & \href{tel:15018331977}
% {\texttt{(510)833-1977}} \\     
% \end{tabular}

%----------------------------------------------------------------------------------------
%	OBJECTIVE SECTION
----------------------------------------------------------------------------------------
\section{\sc Academic appointments}
{\bf FODSI Postdoctoral Fellow} \quelle{{2021---}}  \vspace{-3mm}
Harvard University, School of Engineering and Applied Sciences, Boston, USA \\
Massachusetts Institute of Technology, Department of EECS, Cambridge, USA \\
% [-3mm]
% \\
% \vspace*{-.1in}
\begin{itemize}\itemsep0em
% \item {\bf University of California, Berkeley}, CA, USA
% \item {\bf University of California, Berkeley}, CA, USA
\item Advisors: \emph{Prof. Susan Murphy \& Prof. Devavrat Shah}
\end{itemize}
 \vspace{-2mm}


\section{
% \faGraduationCap\ 
\sc Education} 
% \faUniversity\ 
{\bf Ph.D., Electrical Engineering and Computer Sciences} \quelle{2015---2021} \vspace{-3mm}
University of California, Berkeley, USA 
\vspace{1mm}

% \\[1mm]
% {\bf Ph.D. in Electrical Engineering and Computer Sciences}
% \\[-3mm]
\begin{itemize}\itemsep0em
\item Advisors: \emph{Prof. Martin Wainwright \& Prof. Bin Yu}
\item Thesis title: 
\href{https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-180.html}
{\emph{Principled statistical approaches for sampling and inference in high dimensions}}
\item Thesis committee members: \emph{Prof. David Aldous \& Prof.
Peter Bartlett}
\end{itemize}


% \faGraduationCap\ 
{\bf  B. Tech., Electrical Engineering }
\quelle{2010---2014}  \vspace{-3mm}
Indian Institute of Technology, Bombay, India 
\vspace{1mm}
% \\
% \\[1mm]
\begin{itemize}
  % \itemsep0em
\item Advisor: \emph{Prof. Vivek Borkar}
\item Graduated with Honors in EE and Minors in Mathematics
\item Secured Institute Rank 1 (amongst a thousand)
\end{itemize}
 % \vspace{-2mm}
%----------------------------------------------------------------------------------------
%	COMPUTER SKILLS SECTION
%----------------------------------------------------------------------------------------
 
%----------------------------------------------------------------------------------------
%	PROFESSIONAL EXPERIENCE SECTION
%----------------------------------------------------------------------------------------
 
\section{
% \faPencilSquareO\ 
\sc Research Interests} 
Theoretical and applied aspects of statistical
machine learning and data science with a focus on
causal inference, reinforcement learning, and
theory of MCMC methods
% \\
% [-2mm]
% \begin{itemize}
%   \item 
% \item High-dimensional statistics, 
% % \item 
% \item Reproducible methodologies for interpretable machine learning
% \end{itemize}

\newcommand{\vspone}{\vspace{-0.1mm}}
\section{
% \faNewspaperO\ 
\sc Achievements \& Awards}

Best Presentation Award, Machine Learning and Statistics Session, Laboratory of Information and Decision Systems (LIDS) Student Conference, MIT
\quelle{USA, 2022}


Best Student Paper Award, Sections on Statistical Computing and Statistical Graphics, American Statistical Association (ASA)
\hfill \emph{USA, 2022}


Outstanding Graduate Student Instructor Award, UC Berkeley 
\hfill \emph{Berkeley, 2020}
% \vspone

Student Travel Award, NeurIPS 2018 \quelle{Canada, 2018}
% \vspone

Oberwolfach Leibniz Graduate Students Travel Grant
\quelle{Germany, 2017}
% \vspone

Student Travel Award, SAMSI QMC Workshop 2017, Raleigh-Durham \quelle{USA,
2017}
% \vspone

Berkeley Fellowship, the most prestigious fellowship for incoming students
\hfill \emph{Berkeley, 2015}
% \vspone

President of India Gold Medal, IIT Bombay, for the highest GPA in the institute
\hfill \emph{India, 2014}
% \vspone

Institute Silver Medal,  IIT Bombay, for the highest Honors GPA in the EE department
\hfill \emph{India, 2014}
% \vspone

Best B. Tech. Project Award, IIT Bombay
\hfill \emph{India, 2014}
% \vspone

All India Rank 10 (amongst half a million), IIT Joint Entrance
Exam (IIT-JEE)
\hfill \emph{India, 2010}
% \vspone

All India Rank 46 (amongst a million), All India Engineering Entrance Exam
\quelle{India, 2010}
% \vspone

\section{\sc Work Experience}
{\bf Microsoft Research}, Research Intern (with Lester Mackey), New England,
USA \hfill\emph{{Summer 2019}}
% \vspone

{\bf Mist Systems} (Juniper Networks), Data Science Intern, Cupertino,
USA \quelle{{Summer 2017}}
% \vspone

 {\bf WorldQuant Research}, Senior Quantitative Researcher, Mumbai, India
\quelle{{2014---2015}}
% \vspone

{\bf Stanford University}, Research intern (with Prof. Balaji Prabhakar),
USA 
\quelle{{Summer 2013}}
% \vspone

{\bf Ivy Mobility}, Data Science Intern,  Chennai, India \quelle{{Winter
2012}}
% \vspone

\newcommand{\eqc}{$^{\star}$}
\newcommand{\alpo}{$^{\dagger}$}
\newcommand{\vsep}{\vspace{-.07in}}


\section{\sc Journal Publications}
{\footnotesize{($\star$ denotes equal contribution, and  $\dagger$ denotes
alphabetical
ordering; title is hyperlinked to the online pdf of the paper)}
% \vspace{-.1in}
}


\begin{enumerate}[label={J\arabic*.},leftmargin=*]
\item\label{mala_jmlr} \textbf{Raaz Dwivedi}\eqc, Yan  Shuo Tan\eqc, Briton Park, Mian Wei,
Kevin Horgan, David Madigan,  and Bin Yu, 
\href{https://arxiv.org/pdf/2008.10109.pdf}
{\ptitle{Stable discovery of interpretable subgroups via
calibration in causal studies}}, \textit{International Statistical
Review (ISR), 2020}.
\item Nick Altieri\alpo, Rebecca L. Barter, James Duncan, \textbf{Raaz Dwivedi},
Karl Kumbier, Xiao Li, Robert Netzorg, Briton Park, Chandan Singh, Yan Shuo
Tan, Tiffany Tang, Yu Wang, Chao Zhang and Bin Yu, \href{https://hdsr.mitpress.mit.edu/pub/p6isyf0g/release/4}
{\ptitle{Curating a COVID-19 data repository
and forecasting county-level death counts in the United States}}, \emph{Harvard Data Science Review (HDSR), 2020}.
\item \textbf{Raaz Dwivedi}\eqc, Nhat Ho\eqc, Koulik Khamaru\eqc, Martin J.
Wainwright,
Michael I. Jordan and Bin Yu, \href{https://arxiv.org/pdf/1810.00828.pdf}
{\ptitle{Singularity, misspecification, and the convergence rate of EM}},
\emph{Annals of Statistics (AoS), 2020}.
\item Yuansi Chen, \textbf{Raaz Dwivedi}, Martin
J. Wainwright and Bin Yu, 
\href{https://www.jmlr.org/papers/volume21/19-441/19-441.pdf}{\ptitle{Fast
mixing of Metropolized Hamiltonian Monte Carlo: Benefits of multi-step gradients}}, \emph{Journal of Machine Learning Research (JMLR), 2020}.
\item \textbf{Raaz Dwivedi}\eqc, Yuansi Chen\eqc, Martin
J. Wainwright and Bin Yu, \href{https://jmlr.csail.mit.edu/papers/volume20/19-306/19-306.pdf}
{\ptitle{Log-concave sampling: Metropolis-Hastings algorithms are fast}}, \emph{Journal of Machine Learning Research (JMLR), 2019}.
\item \textbf{Raaz Dwivedi}\alpo, Ohad N. Feldheim, Ori Gurel-Gurevich and
Aaditya Ramdas, \href{https://link.springer.com/article/10.1007/s00440-018-0860-y}{\ptitle{The power of online thinning in reducing discrepancy}}, \emph{Probability Theory and Related Fields (PTRF), 2019}.
\item Yuansi Chen\eqc, \textbf{Raaz Dwivedi}\eqc, Martin
J. Wainwright and Bin Yu, \href{https://jmlr.org/papers/volume19/18-158/18-158.pdf} {\ptitle{Fast MCMC sampling algorithms on polytopes}}, \textit{Journal of Machine Learning Research (JMLR), 2018}. 
\item Vivek Borkar\alpo, \textbf{Raaz Dwivedi} and Neeraja Sahasrabudhe,
\href{https://www.sciencedirect.com/science/article/abs/pii/S016769111600058X}{
\ptitle{Gaussian approximations in high dimensional estimation}},
\emph{Systems \& Control Letters, 2016}.
\end{enumerate}




\section{\sc Conference Publications}

\begin{enumerate}[label={C\arabic*.},leftmargin=*]
  % \setcounter{enumi}{4}
\item \textbf{Raaz Dwivedi} and Lester Mackey, \href{https://arxiv.org/abs/2110.01593}{\ptitle{{Generalized kernel thinning}}}, To appear in \textit{International Conference on Learning Representations (ICLR), 2022}.
\item Abhishek Shetty, \textbf{Raaz Dwivedi} and Lester Mackey, \href{https://arxiv.org/abs/2111.07941}{\ptitle{{Distribution compression in near-linear time}}},  To appear in \textit{International Conference on Learning Representations (ICLR), 2022}.
\item \textbf{Raaz Dwivedi} and Lester Mackey, \href{https://arxiv.org/abs/2105.05842}{\ptitle{{Kernel thinning}}}, Extended abstract in \emph{Conference
on Learning Theory (COLT), 2021}.
\item \textbf{Raaz Dwivedi}\eqc, {Nhat Ho}\eqc, Koulik Khamaru\eqc, Martin
J. Wainwright, Michael I. Jordan and Bin Yu, \href{https://arxiv.org/pdf/1902.00194.pdf} {\ptitle{Sharp analysis of Expectation-Maximization for weakly identifiable models}},  \emph{ The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS), 2020}.
\item \textbf{Raaz Dwivedi}\eqc, {Nhat Ho}\eqc, Koulik Khamaru\eqc,
Martin
J. Wainwright and
Michael I. Jordan, \href{https://people.eecs.berkeley.edu/~minhnhat/EM_misspecified_unified.pdf}
{\ptitle{Theoretical guarantees for EM under misspecified Gaussian mixture
models}},
\emph{Advances in Neural Information Processing Systems (NeurIPS), Montre{\'a}l, 2018}.
\item \textbf{Raaz Dwivedi}\eqc, Yuansi Chen\eqc, Martin
J. Wainwright and Bin Yu, \href{https://jmlr.csail.mit.edu/papers/volume20/19-306/19-306.pdf}
{
\ptitle{Log-concave sampling:
Metropolis-Hastings algorithms are fast}}, Extended abstract in \emph{Conference
on Learning
Theory (COLT), Stockholm, 2018}. 
 % {\footnotesize{(Full version in JMLR~\ref{mala_jmlr})}}
\item Yuansi Chen\eqc, \textbf{Raaz Dwivedi}\eqc, Martin
J. Wainwright and Bin Yu, \href{https://jmlr.org/papers/volume19/18-158/18-158.pdf}
{\ptitle{Vaidya walk: A sampling algorithm based on the volumetric barrier}},
\textit{Communication, Control, and Computing (Allerton),
55th Annual Allerton Conference, 2017}.
\item \textbf{Raaz Dwivedi} and Vivek Borkar, \href{https://ieeexplore.ieee.org/abstract/document/6983986/}
{\ptitle{Removing sampling bias in networked stochastic approximation}},
\textit{International
Conference on Signal Processing and Communications (SPCOM), Bangalore, 2014}.
\end{enumerate}

\section{\sc Pre-Prints} 

\begin{enumerate}[label={P\arabic*.},leftmargin=*]
\item \textbf{Raaz Dwivedi}, Susan Murphy, and Devavrat Shah, \href{https://arxiv.org/abs/2202.06891}{\ptitle{Counterfactual inference in sequential experimental design}}, \textit{arXiv preprint}.
\item \textbf{Raaz Dwivedi}\eqc, {Chandan Singh}\eqc, Bin Yu and Martin
J. Wainwright, \href{https://arxiv.org/abs/2006.10189}{\ptitle{Revisiting minimum description length complexity in overparameterized models}}, \textit{arXiv preprint (in journal submission)}.
\item Nhat Ho\eqc, Koulik Khamaru\eqc, \textbf{Raaz Dwivedi}\eqc, Martin
J. Wainwright, Michael I. Jordan and Bin Yu,  \href{https://arxiv.org/abs/2005.11411}
{\ptitle{Instability, computational efficiency,
and statistical accuracy}}, \textit{arXiv preprint (in journal submission)}.
\end{enumerate}


% \section{\sc Refereed Workshop papers}


% \section{\sc Workshop Organization} 

% \textit{Integration of Deep Learning Theories} at NeurIPS 2018, Palais des Congrès de Montréal, Canada.

% {\small Co-organize with Professors Richard Baraniuk, Stephane Mallat, Anima Anandkumar, and Ankit Patel}


\section{\sc Research Talks}

\begin{enumerate}[label={T\arabic*.},leftmargin=*]\itemsep0.1em
    \item Counterfactual inference in sequential experimental design. \emph{Learning from Interventions Workshop, Simons Institute, Berkeley}. (Invited talk) \quelle{Feb 2022}
    \item Near-optimal compression in near-linear time. \emph{27th Annual Laboratory for Information \& Decision Systems (LIDS) Student Conference, MIT}. (Contributed talk) \quelle{Jan 2022}
    \item Imputation using nearest neighbors for adaptively collected data. \emph{Foundations of Data Science Institute (FODSI) Retreat}. (Invited talk)  \quelle{Jan 2022}
    \item Revisiting Minimum Description Length Complexity in Overparameterized Models. \emph{Collaborations on the Theoretical Foundations of Deep Learning}. (Invited talk)  \quelle{Nov 2021}

    \item Non-asymptotic Guarantees for MCMC and Kernel Thinning. \emph{Finale Doshi-Velez Group Meeting, Harvard University}. (Invited talk) \quelle{Oct 2021}

    \item Kernel Thinning. \emph{ The Data-Centric Engineering Reading Group (DCE), Alan Turing Institute}. (Invited talk) \quelle{Sep 2021}

    \item Kernel Thinning. \emph{Stat 300, Harvard University}. \quelle{Sep 2021}

    \item Kernel Thinning. \emph{Monte Carlo Methods \& Applications (MCM)}. (Contributed talk) \hfill\emph{Sep 2021}

    \item Kernel Thinning. \emph{2021 World Meeting of the International Society for Bayesian Analysis (ISBA)}. (Contributed talk) \quelle{Aug 2021}

    \item Kernel Thinning. \emph{The Bayesian Young Statisticians Meeting (BAYSM) 2021}. (Contributed talk) \quelle{Aug 2021}

    \item Kernel Thinning. \emph{Conference on Learning Theory (COLT)}. (Contributed talk) \quelle{Aug 2021}

    \item Kernel Thinning. \emph{Subset Selection in Machine Learning Workshop, International Conference on Machine Learning (Subset ML, ICML)}. (Contributed talk) \quelle{July 2021}

    \item Revisiting Complexity and the Bias-Variance Tradeoff: Using Minimum Description Length. \emph{North American School of Information Theory (NASIT)}. (Contributed poster)  \quelle{June 2021}

    \item Revisiting Complexity and the Bias-Variance Tradeoff: Using Minimum Description Length. \emph{Workshop on the Theory of Overparameterized Machine Learning (TOPML)}. (Contributed talk)  \quelle{Apr 2021}

    \item Revisiting Complexity and the Bias-Variance Tradeoff: Using Minimum Description Length. \emph{Stat 212, UC Berkeley}. (Guest Lecture)  \quelle{Apr 2021}

    \item Subgroup Discovery in Randomized Experiments \& Markov Chain Monte Carlo Sampling. \emph{Research Seminar, USC Marshall School of Business}. (Invited talk) \quelle{Feb 2021}
    \item Subgroup Discovery in Randomized Experiments \& Markov Chain Monte Carlo Sampling. \emph{Statistics Seminar, University of Toronto}. (Invited talk) \quelle{Feb 2021}
    \item Subgroup Discovery in Randomized Experiments \& Markov Chain Monte Carlo Sampling. \emph{MINDS Symposium on the Foundations of Data Science, John Hopkins University}. (Invited talk) \quelle{Feb 2021}
    \item Subgroup Discovery in Randomized Experiments \& Markov Chain Monte Carlo Sampling. \emph{Devavrat Shah and Susan Murphy Group Meetings, MIT and Harvard University}. (Invited talk) \quelle{Feb 2021}
    \item Subgroup Discovery in Randomized Experiments \& Markov Chain Monte Carlo Sampling. \emph{Research Seminar, Microsoft Research New England}. (Invited talk) \quelle{Jan 2021}
    \item Non-asymptotic Guarantees for Markov Chain Monte Carlo. \emph{Flatiron Institute Seminar}. (Invited talk) \quelle{Jan 2021}
    \item Subgroup Discovery in Randomized Experiments \& Markov Chain Monte Carlo Sampling. \emph{Statistics Seminar, University of Washington}. (Invited talk) \quelle{Jan 2021}
    \item Subgroup Discovery in Randomized Experiments \& Markov Chain Monte Carlo Sampling.  \emph{Operations Research and Statistics Group Seminar, MIT Sloan}. (Invited talk) \quelle{Jan 2021}
  \item New Perspectives on Old Problems in Causal Inference and MCMC Sampling. \emph{Statistics Seminar, UC Irvine}. (Invited talk) \quelle{Jan 2021}
  \item StaDISC: Stable discovery of interpretable subgroups via calibration.
  \emph{Young Data Scientist Research Seminar, ETH
  Zurich}. (Invited talk) \quelle{Sep 2020}

  \item Veridical Data Science and the PCS Framework. \emph{ASA  Annual
  Symposium on Data Science and Statistics (SDSS)}.
  (Invited talk)  \quelle{Jun 2020}

  % https://ww2.amstat.org/meetings/sdss/2020/onlineprogram/AbstractDetails.cfm?AbstractID=308413
% http://www.ams.org/meetings/sectional/2266_program_ss16.html
  
  \item Statistics Meets Optimization: Two Vignettes on The Intersection,
  \emph{Department of Mathematics and Statistics, IIT Kanpur, India}. (Invited
  talk)
  \quelle{Jan
  2020}

  \item Singularity, misspecification and the convergence rate of Expectation-Maximization.
  \emph{Fall Western Sectional Meeting of the AMS, UC Riverside}.
  (Invited talk) \quelle{Nov 2019}

  \item Power of gradients and accept-reject step in MCMC algorithms. 
  \emph{BIDS Statistics and Machine Learning Forum, UC Berkeley}. (Invited
  Talk) \quelle{Mar 2019}

  \item Log-concave sampling: Metropolis Hastings algorithms are fast. 
  \emph{Conference on Learning Theory (COLT) 2018, Stockholm, Sweden}. 
  (Conference Poster) \quelle{Dec 2018}

  \item Log-concave sampling: Metropolis Hastings algorithms are fast. 
  \emph{Jerusalem Joint Statistical Event, Israel}. (Contributed talk) 
  \quelle{Dec 2018}

  \item Theoretical guarantees for EM under misspecified Gaussian mixture
  models. \emph{Neural Information Processing Systems (NeurIPS) 2018, 
  Montre{\'a}l, Canada}. (Conference Poster)
  \quelle{Dec 2018}
  \item Theoretical Guarantees for MCMC Algorithms,  \emph{Department of
  Electrical Engineering, IIT Bombay, India}. (Invited talk)
  \quelle{Jan 2018}
  \item Theoretical Guarantees for MCMC Algorithms, \emph{School of Technology
  and Computer Science Seminar, TIFR Bombay, India}. (Invited talk) \quelle{Jan 2018}
  \item The power to two choices in reducing discrepancy, \emph{SAMSI QMC
  Opening Workshop, Raleigh-Durham, Duke University.}  
  (Contributed Poster) \quelle{Aug 2017}
\end{enumerate}

\section{\sc Teaching Experience}
\textbf{Teaching Fellow, Harvard University} \quelle{2022--}
\begin{itemize}
  \item \emph{STAT 234, Spring 2022}: Sequential Decision Making, taught by Prof. Susan Murphy. 
\end{itemize}
\textbf{Graduate Student Instructor, UC Berkeley} \quelle{2018---2019}
\begin{itemize}
\item \emph{EECS 189, Spring 2018}: Introduction
to Machine Learning, taught by Prof. Anant Sahai and Prof. Jennifer Listgarten.
\emph{Co-led} the content development (homeworks, discussions and exams)
in a team
of 20+ TAs in a class of 350+ students.

% \vspace{-1mm}
% \begin{itemize}\itemsep0em
%   \item Student evaluation score of 4.67/5 (avg. 4.29)
%   \item Taught a lecture (90 mins) on ``An Introduction to Ensemble
%   Methods: Bagging, Random Forest, Boosting'' (invited by Prof. Jennifer
%   Listgarten) in Fall 2019.
% \end{itemize}
\item Spring 2019 for \emph{STAT 154}:  Modern Statistical Prediction and
Machine
Learning taught by Prof. Bin Yu.
Helped in \emph{redesigning} the class along with one other TA, Yuansi Chen,
for a class of 140+ students.
% \begin{itemize}\itemsep0em
%   \item Student evaluation score of 6.46/7 (avg. 5.48)
%   \item Taught a lecture (90 mins) on ``Boosting'' (invited by Prof. Bin
%   Yu) in Fall 2019.
% \end{itemize}
\item \textbf{Guest lectures}: 
% \begin{enumerate}[label=\roman*.]
(i) \emph{STAT 154, Spring 2019}: ``Boosting''
 (ii) \emph{EECS 189, Fall 2019}: ``An Introduction to Ensemble
  Methods: Bagging, Random Forest, Boosting''.
 % \end{enumerate} 
\end{itemize}
% \vspace{-3mm}

\textbf{Teaching Assistant, IIT Bombay and Government of India} \quelle{2011---2014}
\begin{itemize}
\item 9-time TA for undergraduate courses on \emph{Calculus, Linear Algebra, Differential Equations, and Electromagnetism}. Responsible for weekly discussions (40 students) besides exam grading.
 % periodic extra sessions with attendance up to 200+ 
% \vspace{-1mm}
\item TA for an online course on \emph{Linear Algebra} (for
400 undergraduate colleges) organized by Ministry of Human Resource Development (MHRD) of Government of India. 
% \begin{itemize}\itemsep0em
%   \item Responsible for weekly discussions (40+ students) besides exam grading
%   \item Introduced Piazza for class discussions in my teaching sections,
%   periodic extra sessions
%   had attendance up to 200+
% \end{itemize}
\end{itemize}
% \vspace{-3mm}

% \textbf{Teaching Assistant, Ministry of Human Resource Development, India}
% \quelle{Spring 2014}
% \begin{itemize}
% \item Worked as a TA for an online course on \emph{Linear Algebra} (for
% 400+ undergraduate
% colleges) organized by Ministry of Human Resource Development of Government
% of India 
% \vspace{-1mm}
% \begin{itemize}\itemsep0em
%   \item Responsible for preparing weekly homework problems and detailed
%   solutions
% \end{itemize}
% \end{itemize}





\section{\sc Academic \\ Services} 

\textbf{Mentoring Activities}

\begin{itemize}\itemsep0em
  \item Postdoc Mentoring Program for Institute for Data, Systems and Society (IDSS) Students, MIT \quelle{2022---}
  \item Berkeley Artificial Intelligence Research (BAIR) PhD Buddy Program for \emph{incoming graduate
  students}, UC Berkeley
  \quelle{2020---2021}
  \item Berkeley Artificial Intelligence Research (BAIR) UG Mentoring Program for \emph{undergraduate students}, UC Berkeley
  \quelle{2017---2021}
  \item Institute Student Mentoring Program (ISMP) for \emph{incoming undergraduate students},
  IIT Bombay
  \quelle{2013---2014}
  \item EE Department Academic Mentoring Program (DAMP) for \emph{sophomores and
  juniors}, IIT Bombay
  \quelle{2012---2014}
  \item Intensive Program for Entrants (IPE) for \emph{incoming undergraduates}, IIT Bombay \hfill{\emph{2012---2013}}
\end{itemize}

\textbf{Reviewing Activities}
\begin{itemize}\itemsep0em
  \item Journal of Machine Learning Research (JMLR) (4 Papers)\hfill{\emph{2020--}}
  \item IEEE Transactions on Information Theory (4 Papers)\hfill{\emph{2020--}}
  \item Bernoulli (1 Paper)\hfill{\emph{2021--}}
  \item International Conference on Machine Learning (ICML) \hfill{\emph{2019,
      2020}}
    \item Neural Information Processing Systems (NeurIPS) \hfill{\emph{2019,
      2020}}
    \item Conference on Learning Theory (COLT) \hfill{\emph{2019}}
    \item Foundations of Computer Science (FOCS) \hfill{
    \emph{2018, 2020}}
    \item Symposium on Discrete Algorithms (SODA) \hfill{
    \emph{2019}}
    \item AAAI Conference on Artificial Intelligence\hfill{\emph{2020}}
\end{itemize}


\textbf{EECS Graduate Admissions Committee}, UC Berkeley
\quelle{2018, 2019, 2020}

\textbf{EECS Graduate Admissions Committee}, MIT
\quelle{2021}


% \section{\sc  References }

% \begin{tabular}{lll}
%   \sc{Bin Yu} & \sc{Martin Wainwright}  &\\
%   Chancellor's Professor,  & Chancellor's Professor, &  \\
%   Dept of EECS and Statistics, & Dept of EECS and Statistics, &  \\
%   University of California, Berkeley & University of California, Berkeley &\\
%   (Ph.D. Advisor) & (Ph.D. Advisor) &\\
%   % \href{mailto:binyu@berkeley.edu}{\texttt{binyu@berkeley.edu}}
%   \href{mailto:binyu@berkeley.edu}{{\footnotesize\faEnvelope}~\small{binyu@berkeley.edu}} &
%   \href{mailto:wainwrig@berkeley.edu}{{\footnotesize\faEnvelope}~\small{wainwrig@berkeley.edu}} 
%   % \href{mailto:wainwrig@berkeley.edu}{\texttt{wainwrig@berkeley.edu}}
%   \\
%  \href{https://binyu.stat.berkeley.edu}{{\faHome}~\small{binyu.stat.berkeley.edu}}
%   & 
%   \href{https://people.eecs.berkeley.edu/~wainwrig}{{\faHome}~\small
% {people.eecs.berkeley.edu/$\sim$wainwrig}}
%   \\ 
%   \\
%   \\
%    \sc{David Madigan}  & \sc{Lester Mackey}\\
%   Professor, Provost
% and  & Principal Researcher \\
% Senior Vice
% President of Academic Affairs,    &Microsoft Research New England \\
%   Khoury College of Computer Sciences   & Adjunct Professor, Dept of Statistics \\
%    Northeastern University &Stanford University \\
%     \href{mailto:d.madigan@northeastern.edu}{{\footnotesize\faEnvelope}~\small{d.madigan@northeastern.edu}} &
%   \href{mailto:lmackey@stanford.edu}{{\footnotesize\faEnvelope}~\small{lmackey@stanford.edu}}
%   \\
%    \href{https://www.khoury.northeastern.edu/people/david-madigan/}{{\faHome}~\small{khoury.northeastern.edu/people/david-madigan}}
%   & 
%   \href{https://web.stanford.edu/~lmackey/}{{\faHome}~\small
% {web.stanford.edu/$\sim$lmackey}}
%    % \href{mailto:d.madigan@northeastern.edu}{\texttt{d.madigan@northeastern.edu}} &  
%    % \href{mailto:lmackey@stanford.edu}{\texttt{lmackey@stanford.edu}}
%    \\
%    \\
%    \\
%    \sc{Kevin Horgan} \\
%    Board Member \\
%    Protypia INC \\
%    Nashville \\
%    \href{mailto:kevinhorgan@icloud.com}{{\footnotesize\faEnvelope}~\small{kevinhorgan@icloud.com}}
%    \\   \href{https://www.linkedin.com/in/kjhorgan/}{\faLinkedin~\small{linkedin.com/in/kjhorgan}}
%    % \href{mailto:kevinhorgan@icloud.com}
%    % {\texttt{kevinhorgan@icloud.com}}
% \end{tabular}

% \textbf{Other services:}
% \begin{itemize}
% \item Nonparametric Statistics Workshop: Integration of Theory, Methods, and Applications, October 2016, Ann Arbor, Michigan, \textit{Student Assistant} 
% \item Extreme Value Analsyis (EVA) conference, June 2015, Ann Arbor, Michigan, \textit{Student Assistant}
% \end{itemize}

% \section{\sc Membership}
% American Statistical Association

% International Society for Bayesian Analysis

\comment{\section{\sc Computer skills}
Programming Languages: C/C++,  Python

Softwares: Mathlab, R, SAS

Operating Systems: Windows, Macintosh. }
%----------------------------------------------------------------------------------------
%	COMMUNITY SERVICE SECTION
%---------------------------------------------------------------------------------------- 
%\section{\sc References}
%\textbf{Richard G. Baraniuk} \\
%Victor E. Cameron Professor of EECS \\
%Rice University, Houston, Texas\\
%Email: richb@rice.edu \\\\
%\textbf{Michael I. Jordan} \\
%Pehong Chen Distinguished Professor of EECS and Statistics \\
%University of Berkeley, California\\
%Email: jordan@cs.berkeley.edu  \\\\
%\textbf{Long Nguyen} \\
%Associate Professor of Statistics \\
%University of Michigan, Ann Arbor\\
%Email: xuanlong@umich.edu \\\\
%\textbf{Natesh Pillai} \\
%Professor of Statistics \\
%Harvard University\\
%Email: pillai@fas.harvard.edu \\\\
%\textbf{Ya'acov Ritov}  \\
%Professor of Statistics\\
%University of Michigan, Ann Arbor\\
%Email: yritov@umich.edu \\\\
%\textbf{Martin J. Wainwright} \\
%Chancellor's Professor of EECS and Statistics \\
%University of Berkeley, California\\
%Email: wainwrig@berkeley.edu   
\end{resume}
\end{document}

